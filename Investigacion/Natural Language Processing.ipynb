{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/usuario/anaconda/lib/python3.6/site-packages/nltk/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'da0ms0'), ('grupo', 'ncms000'), ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hola Sr. Smith, ¿cómo te va hoy? El clima es genial, y Python es increíble. El cielo es rosado-azul. Usted no debe comer cartón.\"\n",
    "\n",
    "print(sent_tokenize(text=EXAMPLE_TEXT, language='spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word_tokenize(text=EXAMPLE_TEXT, language='spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"Esta es una frase de ejemplo, mostrando las palabras de parada de filtración.\"\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent, language='spanish')\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "example_words = [\"parada\",\"parador\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "example_words = [\"Automovilista\",\"Automovil\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(stemmer.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = \"Es importante por ser muy programador mientras que usted está programa con programa. Todos los programadores han programado mal por lo menos una vez.\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(stemmer.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text = 'En una entrada anterior os hablaba de un proyecto que llevo estudiando desde hace algún tiempo para el análisis de emociones, dicho proyecto es “wefeelfine” y tal como os comentaba, cuenta con una API Rest que permite realizar varios tipos de consultas para obtener un listado de sentimientos recolectados en Internet. La recolección de datos que realiza dicha plataforma se basa en la ejecución de varios procesos de crawling para extraer y almacenar información de múltiples sitios en Internet, sin embargo, una de las cosas que más me ha llamado la atención es el procesamiento de los textos y las funciones utilizadas para determinar la polaridad de dichas palabras. La complejidad de un proceso de “stemming” (derivación y combinaciones de palabras) sobre el lenguaje natural no es una tarea sencilla, especialmente cuando hablamos de las características propias de cada lenguaje, al final ocurre que solamente nos podemos centrar en un conjunto limitado de lenguajes, tal como ocurre en “wefeelfine” que solo procesa los textos escritos en ingles y en castellano, aunque este último con bastantes deficiencias. Si estás pensando en desarrollar un proceso de minería de datos sobre alguna red social como Twitter, Facebook o cualquier otro sitio web en Internet, lo más probable es que te interese también extraer información útil a la que posteriormente le puedas aplicar algún tipo de análisis estadístico sobre marcas, tendencias o de carácter histórico/evolutivo. En cualquier caso, es necesario aplicar técnicas de “Procesamiento del lenguaje Natural” o NLP (Natural Lenguage Processing) por sus siglas en ingles, las cuales permiten aplicar patrones a un texto concreto y extraer información de interés (no confundir con NLP: Neuro-Linguistic Programming). Antes de continuar, viene bien explicar en qué consiste NLP y cómo se puede utilizar la librería NLTK en Python.'#state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = 'Se trata de un conjunto de técnicas que permiten el análisis y manipulación del lenguaje natural. Dada la complejidad intrínseca que acompaña cualquier proceso de NLP, muchas de las técnicas automatizadas están relacionadas con el uso de la IA (Inteligencia Artificial) y las ciencias cognitivas. Para aplicar las técnicas más comunes en NLP, la librería NLTK (Natural Language Tool Kit) permite que cualquier programa escrito en lenguaje Python pueda invocar a un amplio conjunto de algoritmos que sustentan las principales técnicas de NLP para la generación de métricas, frecuencia de términos, polaridad negativa/positiva de frases y textos, entre otras muchas técnicas.Existen algunos términos comunes en NLP que se deben comprender antes de poder aplicar cualquier técnica y entender los resultados que arrojan, dichos términos resultarán de lo más lógicos, pero si no se tienen en cuenta a la hora de programar, pueden resultar confusas las funciones y resultados arrojados por NLTK.'#state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    #sets = nltk.corpus.cess_esp.tagged_words()\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words,lang='spa')\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp as cess\n",
    "symbols = list(set(w[0].lower() for s in cess.tagged_sents() for w in s))\n",
    "list(filter(lambda x: \"cabo\" in x, symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp as cess\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "spanish_sentence_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trigram_tagger(training_set):\n",
    "   default_tagger = nltk.DefaultTagger('NN')\n",
    "   unigram_tagger = nltk.UnigramTagger(training_set, backoff=default_tagger)\n",
    "   bigram_tagger = nltk.BigramTagger(training_set, backoff=unigram_tagger)\n",
    "   return nltk.TrigramTagger(training_set, backoff=bigram_tagger)\n",
    "\n",
    "def tag_sentences(tagger, sentence_tokenizer, sentences):\n",
    "   tokenized_sentences = sentence_tokenizer.tokenize(sentences)\n",
    "   return [tagger.tag(word_tokenize(s)) for s in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Entre no el modelo\n",
    "tagger = trigram_tagger(cess.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pruebo el modelo\n",
    "texto = \"Poner un botón en la pantalla de tramites\"\n",
    "print(tag_sentences(tagger, spanish_sentence_tokenizer, texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_sentences2(tagger, sentence_tokenizer, sentences):\n",
    "    tokenized_sentences = sentence_tokenizer.tokenize(sentences)\n",
    "    try:\n",
    "        for i in tokenized_sentences:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = tagger.tag(words)\n",
    "            \n",
    "            #busco el patron\n",
    "            chunkGram = r\"\"\"Accion: {<v.*> <d.*> <n.*> <s.*> <N.*> <s.*> <d.*> <n.*> <N.*>+}\n",
    "                                    {<v.*> <d.*> <n.*> <s.*> <d.*> <n.*> <s.*> <N.*>}\n",
    "                                    {<v.*> <d.*> <n.*> <s.*> <N.*> <s.*> <d.*> <n.*>+}\n",
    "                                    {<v.*> <d.*> <n.*> <s.*> <d.*> <n.*>+} \n",
    "                                    {<v.*> <d.*> <n.*> <s.*> <d.*> <n.*>+}                                    \n",
    "                                    {<v.*> <d.*> <N.*> <s.*> <d.*> <N.*>+}\n",
    "                                    {<v.*> <d.*> <n.*>+}                                    \n",
    "                                    \"\"\"\n",
    "            \n",
    "            # chunkGram = r\"\"\"Accion: {<v.*> <d.*> <n.*>+}\"\"\"\n",
    "            \n",
    "            # elimino del patron\n",
    "            # chunkGram = r\"\"\"Accion: {<.*>+} \n",
    "            #                         }<d.*|s.*|c.*|r.*>{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Accion'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "               \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texto = \"Poner un botón en la pantalla de tramites y luego eliminar el filtro de cuil de la pantalla Pensiones y quitar el titulo a la pagina\"\n",
    "tag_sentences2(tagger, spanish_sentence_tokenizer, texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_sentences3(tagger, sentence_tokenizer, sentences):\n",
    "    tokenized_sentences = sentence_tokenizer.tokenize(sentences)\n",
    "    for i in tokenized_sentences:\n",
    "        words = nltk.word_tokenize(i, language='spanish')\n",
    "        tagged = tagger.tag(words)\n",
    "        namedEnt = nltk.ne_chunk(tagged, binary=False)\n",
    "        namedEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texto = \"Poner un botón en la pantalla de Argentina y luego eliminar el filtro de cuil de la pantalla Pensiones y quitar el titulo a la pagina\"\n",
    "tag_sentences3(tagger, spanish_sentence_tokenizer, texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:10]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=False)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d5fdc4a828a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mEjemplo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Esta es la casa de Maria. Aca vive ella con su familia.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgold_chunked_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEjemplo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/usuario/anaconda/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36msubtrees\u001b[0;34m(self, filter)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mall\u001b[0m \u001b[0mlocal\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree\n",
    "\n",
    "tagged_text = \"[ The/DT cat/NN ] sat/VBD on/IN [ the/DT mat/NN ] [ the/DT dog/NN ] chewed/VBD ./.\"\n",
    "gold_chunked_text = tagstr2tree(tagged_text)\n",
    "unchunked_text = gold_chunked_text.flatten()\n",
    "\n",
    "Ejemplo = \"Esta es la casa de Maria. Aca vive ella con su familia.\"\n",
    "texto = gold_chunked_text.subtrees(Ejemplo)\n",
    "for t in texto:\n",
    "    print(str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'train_tagger.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
